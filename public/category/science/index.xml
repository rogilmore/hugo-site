<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>science | Rick Gilmore&#39;s site</title>
    <link>/category/science/</link>
      <atom:link href="/category/science/index.xml" rel="self" type="application/rss+xml" />
    <description>science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018-2020</copyright><lastBuildDate>Wed, 20 Mar 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>science</title>
      <link>/category/science/</link>
    </image>
    
    <item>
      <title>Open developmental science</title>
      <link>/post/open-developmental-science/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/open-developmental-science/</guid>
      <description>&lt;p&gt;If you plan to attend the Society for Research in Child Development (SRCD) meeting in Baltimore later this week, you might be interested in some of the open science activities that will be taking place:&lt;/p&gt;
&lt;p&gt;On Friday, March 22, from 10-11:30 am, I am co-leading a conversation hour on the topic 
&lt;a href=&#34;https://www.rick-gilmore.com/talk/srcd-2019-03-22/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;What SRCD is doing to address open science&lt;/em&gt;&lt;/a&gt;.
We&amp;rsquo;ll focus on a set of new policies and guidelines to authors that the SRCD Task Force on Scientific Integrity and Openness produced.&lt;/p&gt;
&lt;p&gt;In addition, there are other open science events happening at SRCD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;ManyBabies: Bringing large-scale collaborative projects to infant development research&lt;/em&gt;, Saturday, March 23, 12:45-2:15 pm, Hilton Baltimore, Level 2, Key 4&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Making developmental science more open: Successes, obstacles, and solutions&lt;/em&gt;, Saturday, March 23, 2:30-4:00 pm, Baltimore Convention Center, Room 309&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See you in Baltimore!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Behavior is the linchpin</title>
      <link>/post/behavior-is-the-linchpin/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/behavior-is-the-linchpin/</guid>
      <description>&lt;p&gt;The Eunice Kennedy Shriver National Institute for Child Health and Human Development (NICHD) has 
&lt;a href=&#34;https://grants.nih.gov/grants/guide/notice-files/NOT-HD-18-031.html?utm_source=2018&amp;#43;Current&amp;#43;Member&amp;#43;List&amp;amp;utm_campaign=09227e70bd-EMAIL_CAMPAIGN_2019_01_17_08_03&amp;amp;utm_medium=email&amp;amp;utm_term=0_e5b504feb0-09227e70bd-293977841&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;requested input&lt;/a&gt; on a draft strategic plan for 2020-2024.&lt;/p&gt;
&lt;p&gt;NICHD supports Databrary and PLAY, and it has supported my habituation modeling work in the past.
The following is a draft response my colleagues and I are working on.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Dear Dr. Bianchi:&lt;/p&gt;
&lt;p&gt;Thank you for the opportunity to respond to the NICHD Strategic Plan for Fiscal Years 2020-2024. We write as Principal Investigators of the Play &amp;amp; Learning Across a Year (PLAY) project (R01HD094830), a 65-researcher-strong video-based study of natural infant/mother behavior, funded by NICHD, NIMH, OBSSSR, and the Office of the Director. Our recommendation is simple: We urge you and your colleagues to incorporate a clear vision for behavioral research in the plan&amp;rsquo;s research themes and goals to underscore the pivotal importance of behavior and experience in child development.&lt;/p&gt;
&lt;p&gt;Behavior is the linchpin of the most vexing problems in public health, and a better understanding of behavior is fundamental to achieving positive health outcomes, from prenatal development throughout adulthood. Behavior contributes to the progression or prevention of disease, defines a disorder or marks recovery, and provides mechanisms for therapeutic intervention. The study of child health and human development magnifies this importance. According to the CDC, two of the three leading causes of death among children aged 1-4 (accidents and assault/homicide) and children aged 5- 14 (accidents and suicide) involve behavior, among other factors. One child in six has a developmental disability. The most common developmental disabilities—autism spectrum disorder (ASD), attention deficit hyperactivity disorder (ADHD), and sensory impairments—substantially impact behaviors across the lifespan, and often the most effective interventions are behavioral. Virtually every problem in children, families, and society—substance use, school violence, school dropout, ethnic disparities, emotional dysregulation, racism and discrimination, and more—can be traced to behaviors and experiences in childhood. Moreover, the growing field of epigenetics reveals that genetics data alone provide limited insights about developmental outcomes without critical information about early experiences, behaviors, and environments.&lt;/p&gt;
&lt;p&gt;In the context of the NICHD 2020-2024 Strategic plan, our specific recommendations are as follows. In Research Theme #1—understanding early human development—we ask that you expand the focus beyond genetic and cellular levels of analysis and incorporate maternal and fetal behavior as central phenomena that need to be studied and understood. Similarly, regarding Research Theme #2—setting the stage for a healthy pregnancy—we ask that you expand the focus to include maternal and other caregiver behaviors, including paternal behaviors, that may reduce or elevate risk.
Research Theme #4—identifying sensitive time periods to optimize health interventions—highlights the role of behavior. However, the plan implies that we have sufficient knowledge about the varied trajectories of typical development to productively intervene in atypical development. This may be true in a limited set of domains, but is not broadly true in our experience. Much more research about behavioral development is needed. Our suggestion about Research Theme #5—Improving health during the transition from adolescence to adulthood—is to broaden the lens to include research on childhood behaviors observed in a variety of settings that serve as precursors to risk in adolescence.&lt;/p&gt;
&lt;p&gt;We wish to emphasize that many standardized laboratory tasks or parent-report questionnaire instruments thought to reliably reflect the &amp;ldquo;phenotype,&amp;rdquo; fail to capture vital dimensions of infant, child, adolescent, and parent behavior that occur in natural settings. Indeed, this gap motivates our focus on video in the PLAY project. Natural behavior in home, school, and clinical settings can be reliably and informatively captured and analyzed using video. And we have shown that video can be securely and ethically shared using tools for video informatics (Databrary and Datavyu) and policies for restricted data sharing with explicit consent that we have developed and shared with the research community thanks to NICHD support (U01HD076595).&lt;/p&gt;
&lt;p&gt;The PLAY project capitalizes on these advances in an effort to capture, analyze, and openly share data about the building blocks of infant and maternal behavior combining video with other data sources. PLAY is motivated in part by the belief that the hard-won knowledge gleaned from public funding for the genome, proteome, metabolome, and connectome initiatives will have substantially greater impact on child and maternal health outcomes when combined with insights from a corresponding &amp;ldquo;behaviorome.&amp;rdquo; As a &amp;ldquo;behaviorome&amp;rdquo;-like research effort, PLAY aims to accelerate the pace of discovery in basic and applied developmental science.
NICHD has a strong track record of funding research on infant and child behavior. Given the essential role of behavior in maternal, child, and adolescent health, we urge NICHD to build upon this tradition by giving the study of behavior substantially greater emphasis and visibility in the 2020-2024 strategic plan and beyond.&lt;/p&gt;
&lt;p&gt;Yours truly,&lt;/p&gt;
&lt;p&gt;Karen Adolph, Professor of Psychology and Neural Science, New York University&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Rick O. Gilmore, Associate Professor of Psychology, The Pennsylvania State University&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;Catherine Tamis-LeMonda, Professor of Applied Psychology, New York University&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>In search of the ethogram</title>
      <link>/post/in-search-of-the-ethogram/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/in-search-of-the-ethogram/</guid>
      <description>&lt;p&gt;Psychology is in part, a science of behavior.
Mental experience &amp;ndash; thoughts and feelings &amp;ndash; are also common targets of inquiry.&lt;/p&gt;
&lt;p&gt;I recently went looking for a systematic catalogue of behaviors that should, in theory, constitute the substance of what psychologists and other behavioral scientists study.
My goal was to take that catalogue as a starting point for thinking about the relationship between information available for perception and the actions (behaviors) that this information informs.
The formal term for this sort of catalog is an &amp;lsquo;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ethogram&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ethogram&lt;/a&gt;&#39;.
Here&amp;rsquo;s a sample 
&lt;a href=&#34;https://tolweb.org/onlinecontributors/app?page=TeacherResourceViewSupportMaterial&amp;amp;service=external&amp;amp;sp=l3090&amp;amp;sp=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;template&lt;/a&gt; from the Tree of Life project.
Unfortunately, I failed in my quest.&lt;/p&gt;
&lt;p&gt;There have been attempts to create and openly share animal ethograms like the apparently stalled or inactive ethobank.org site associated with the EthoSource project.
The 
&lt;a href=&#34;https://www.cogpo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cognitive Paradigm Ontology&lt;/a&gt; and the 
&lt;a href=&#34;https://www.cognitiveatlas.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cognitive Atlas&lt;/a&gt; project attempt to create systematic ontologies (systematic representations of categories and concepts and their relations) of tasks used in experimental psychology.
Cognitive psychology tasks clearly require very specific types of behavior from participants.
Systematically organizing the nature of these tasks should lead to faster progress in understanding how they relate to one another, and ultimately, how minds and brains actually work.
Still, I came up short in finding a comprehensive human ethogram.&lt;/p&gt;
&lt;p&gt;In other contexts, I&amp;rsquo;ve called such a thing the behaviorome, partly to suggest that behavior should have a more central place in research grantmaking priorities.
The 
&lt;a href=&#34;/project/PLAY/&#34;&gt;PLAY project&lt;/a&gt; is a type of behaviorome effort.
We will be creating detailed definitions for specific behaviors that our infant and adult participants produce in a small set of behavioral domains, but it will not be comprehensive by any means.
For now, I&amp;rsquo;m stuck with trying to assemble my own.&lt;/p&gt;
&lt;p&gt;One approach to an ethogram for humans could focus on biologically essential behavioral classes that we share with all other animals: &lt;em&gt;ingestion&lt;/em&gt; (food and nutrient seeking), &lt;em&gt;defense&lt;/em&gt; (self-protection), and &lt;em&gt;reproduction&lt;/em&gt;.
Unfortunately, this scheme fail to capture the sets of behaviors studied by the majority of psychologists.
Adding in a class of communicative and affiliative behaviors helps.
But even this augmented classification describes the functional purposes of behavior, not the behaviors themselves.
For example, locomotor behavior, moving toward or away from some target, occurs in food seeking, predator avoidance, mate seeking, and social (affiliative) contexts.&lt;/p&gt;
&lt;p&gt;Another way to ask the question is to observe whether physical activities that humans do for pleasure and watch others for entertainment (sports) have some latent structure we might exploit.
Here, we assume that if large groups of humans engage in an activity or pay to watch others engage in it, the skills involved must be especially demanding.
Indeed, professional sports represent some of humankind&amp;rsquo;s most skilled (and highly paid) actors.
And while I&amp;rsquo;m still looking for a comprehensive catalog of sport types, this table shows that most of them fall into a small set of categories when focusing on the core actions that define success.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Sports&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;aiming&lt;/td&gt;
&lt;td&gt;basketball, baseball, football (passing &amp;amp; kicking), hocky, soccer, bowling, tennis, golf, pool/billiards, archery, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;steering&lt;/td&gt;
&lt;td&gt;slalom skiing, motorsports&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;performing&lt;/td&gt;
&lt;td&gt;gymnastics, diving, synchronized swimming, figure skating, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;racing&lt;/td&gt;
&lt;td&gt;running, motorsports, speed skating, swimming, cycling, paddling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fighting&lt;/td&gt;
&lt;td&gt;boxing, wrestling, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;feats of strength&lt;/td&gt;
&lt;td&gt;weightlifting&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You may disagree with my categorization, but I hope you&amp;rsquo;ll agree that there are many similarities in the sorts of actions we find entertaining to watch.&lt;/p&gt;
&lt;p&gt;Another approach to an ethogram starts with the actions that computer vision researchers find are important to extract from video.
Google’s Atomic Visual Action (AVA) dataset (&lt;a href=&#34;https://research.google.com/ava/&#34;&gt;https://research.google.com/ava/&lt;/a&gt;) involves three classes of movements, &lt;em&gt;person movement&lt;/em&gt;, &lt;em&gt;object interaction&lt;/em&gt;, &lt;em&gt;person interaction&lt;/em&gt;, and has exemplars with 81 specific actions that fit into these categories.
Some examples of specific behaviors tagged in the AVA dataset are &lt;em&gt;crawl&lt;/em&gt;, &lt;em&gt;dance&lt;/em&gt;, &lt;em&gt;jump/leap&lt;/em&gt;, &lt;em&gt;lift&lt;/em&gt;, &lt;em&gt;open&lt;/em&gt;, &lt;em&gt;press&lt;/em&gt;, &lt;em&gt;hug&lt;/em&gt;, &lt;em&gt;kiss&lt;/em&gt;, and &lt;em&gt;listen to&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Maybe some combination of these approaches is as good a start as any.
After all, if 
&lt;a href=&#34;https://doi.org/10.1016/j.anbehav.2009.03.018&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;behavioral biologists don&amp;rsquo;t seem to agree about what constitutes behavior&lt;/a&gt;, it&amp;rsquo;s not that surprising that psychologists haven&amp;rsquo;t yet created our own list.
We do need a list like this, though, don&amp;rsquo;t you think?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parameters for action</title>
      <link>/post/parameters-for-action/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/parameters-for-action/</guid>
      <description>


&lt;p&gt;Several years ago, Florian Raudies, Swapnaa Jayaraman and I published a &lt;a href=&#34;https://doi.org/10.1109/DEVLRN.2015.7345450&#34;&gt;paper&lt;/a&gt; where we simulated the optic flow that infants would experience in different head/body postures.
We computed cyclopian (one-eyed) flow on the basis of this schematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gilmore-lab/temple-2017-02-27/master/img/computing-flow.jpg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Here, the key parameters were the instantaneous translation &lt;span class=&#34;math inline&#34;&gt;\((v_x{}, v_y{}, v_z{})\)&lt;/span&gt; and rotation &lt;span class=&#34;math inline&#34;&gt;\((\omega_{x}, \omega_y{}, \omega_z{})\)&lt;/span&gt; of the planar retina.
Coupled with the optic flow equation,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{pmatrix}\dot{x} \\ \dot{y}\end{pmatrix}=\frac{1}{z} \begin{pmatrix}-f &amp;amp; 0 &amp;amp; x\\ 0 &amp;amp; -f &amp;amp; y \end{pmatrix} \begin{pmatrix}{v_x{}}\\ {v_y{}} \\{v_z{}}\end{pmatrix}+ \frac{1}{f} \begin{pmatrix} xy &amp;amp; -(f^2+x^2) &amp;amp; fy\\ f^2+y^2 &amp;amp; -xy &amp;amp; -fy \end{pmatrix} \begin{pmatrix} \omega_{x}\\ \omega_{y}\\ \omega_{z} \end{pmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we were able to simulate the &lt;em&gt;perceptual&lt;/em&gt; effects of postural geometry: Changes in eye height and forward translational speed that would occur when a child changed from crawling to walking altered the pattern of retinal flow &lt;span class=&#34;math inline&#34;&gt;\((\dot{x}, \dot{y})\)&lt;/span&gt; in interesting ways.&lt;/p&gt;
&lt;p&gt;This work has lain dormant for a few years, but I now want to pick it back up.
In short, there are a handful of perception/action systems that provide the nervous system with deterministic, causal information about the effects of different actions.
These must be important for development.&lt;/p&gt;
&lt;p&gt;For the next step, I’m looking for a concise, but thorough parameterization of body posture that includes the eyes, head, torso, arms, and legs.
Here’s a sketch of what I have in mind for the upper parts body that have the greatest impact on the direction of visual fixation:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Body part&lt;/th&gt;
&lt;th&gt;Parameter(s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Eyes&lt;/td&gt;
&lt;td&gt;$&lt;em&gt;{rx}, &lt;/em&gt;{ry}, &lt;em&gt;{lx}, &lt;/em&gt;{lx} $&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Head&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_x{}, \theta_y{}, \theta_z{}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Torso&lt;/td&gt;
&lt;td&gt;$_x{}, _y{}, _z{} $&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Coupled with the distance between the eyes, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the radial distance to the head’s center of rotation, &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, and the distance from the head’s center of rotation to the torso’s center of rotation, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, we can compute the effects of eye, head, and torso movement on visual motion at the two retinae.
Now, if the &lt;em&gt;visual&lt;/em&gt; signals from eye vs. head vs. torso can be distinguished, then these could couple with other proprioceptive (muscle, tendon, cutaneous) signals to provide a powerful set of &lt;em&gt;sensory&lt;/em&gt; signals that are directly caused by eye, head, and torso motion.
See &lt;a href=&#34;../the-webs-we-weave/&#34;&gt;this earlier post&lt;/a&gt; for a causal graph that elaborates on this point.
I’ll discuss why I think there are &lt;em&gt;visual&lt;/em&gt; differences in the effects of eye and head motion in a future post.&lt;/p&gt;
&lt;p&gt;My next step is to ask my colleagues in kinesiology if there is a canonical parameterization of body position that I can build upon.
If you know of one, let me know.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The webs we weave</title>
      <link>/post/the-webs-we-weave/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/the-webs-we-weave/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve been ‘sketching’ out conceptual models for some time without realizing that there is an entire field of analysis where these sorts of diagrams are the starting point: Causal modeling. My introduction to this world was Judea Pearl’s &lt;a href=&#34;https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1548436587&amp;amp;sr=8-1&amp;amp;keywords=the+book+of+why&#34;&gt;The Book of Why: The New Science of Cause and Effect&lt;/a&gt;. Interested casual readers might also enjoy Steve Sloman’s excellent and delightfully clear &lt;a href=&#34;https://www.amazon.com/Causal-Models-People-Think-Alternatives/dp/0195394291/ref=sr_1_2?ie=UTF8&amp;amp;qid=1548436730&amp;amp;sr=8-2&amp;amp;keywords=steven+sloman&#34;&gt;&lt;em&gt;Causal Models: How People Think About the World and Its Alternatives&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I want to build a model for how an observer perceives visual motion given various possible causes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DiagrammeR)

nodes &amp;lt;- create_node_df(
  n = 15,
  label = c(&amp;quot;retinal motion&amp;quot;, &amp;quot;eye mvmt&amp;quot;, &amp;quot;head mvmt&amp;quot;, &amp;quot;body mvmt&amp;quot;, &amp;quot;object mvmt&amp;quot;, &amp;quot;passive transp&amp;quot;, &amp;quot;eye cmd&amp;quot;, &amp;quot;head cmd&amp;quot;, &amp;quot;body cmd&amp;quot;, &amp;quot;eye prop&amp;quot;, &amp;quot;head prop&amp;quot;, &amp;quot;head vest&amp;quot;, &amp;quot;body prop&amp;quot;, &amp;quot;entity&amp;quot;, &amp;quot;passive force&amp;quot;),
  type = &amp;quot;number&amp;quot;
)

edges &amp;lt;- create_edge_df(
  from = c(2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 3, 4, 4, 14, 15),
  to = c(1, 1, 1, 1, 4, 2, 3, 4, 10, 11, 12, 12, 13, 5, 5),
  rel = &amp;quot;related&amp;quot;
)

graph &amp;lt;- DiagrammeR::create_graph(nodes_df = nodes, 
                                  edges_df = edges)

graph %&amp;gt;% DiagrammeR::render_graph()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\ngraph [layout = \&#34;neato\&#34;,\n       outputorder = \&#34;edgesfirst\&#34;,\n       bgcolor = \&#34;white\&#34;]\n\nnode [fontname = \&#34;Helvetica\&#34;,\n      fontsize = \&#34;10\&#34;,\n      shape = \&#34;circle\&#34;,\n      fixedsize = \&#34;true\&#34;,\n      width = \&#34;0.5\&#34;,\n      style = \&#34;filled\&#34;,\n      fillcolor = \&#34;aliceblue\&#34;,\n      color = \&#34;gray70\&#34;,\n      fontcolor = \&#34;gray50\&#34;]\n\nedge [fontname = \&#34;Helvetica\&#34;,\n     fontsize = \&#34;8\&#34;,\n     len = \&#34;1.5\&#34;,\n     color = \&#34;gray80\&#34;,\n     arrowsize = \&#34;0.5\&#34;]\n\n  \&#34;1\&#34; [label = \&#34;retinal motion\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34; [label = \&#34;eye mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;3\&#34; [label = \&#34;head mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;4\&#34; [label = \&#34;body mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;5\&#34; [label = \&#34;object mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;6\&#34; [label = \&#34;passive transp\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;7\&#34; [label = \&#34;eye cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;8\&#34; [label = \&#34;head cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;9\&#34; [label = \&#34;body cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;10\&#34; [label = \&#34;eye prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;11\&#34; [label = \&#34;head prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;12\&#34; [label = \&#34;head vest\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;13\&#34; [label = \&#34;body prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;14\&#34; [label = \&#34;entity\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;15\&#34; [label = \&#34;passive force\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34;-&gt;\&#34;1\&#34; \n  \&#34;3\&#34;-&gt;\&#34;1\&#34; \n  \&#34;4\&#34;-&gt;\&#34;1\&#34; \n  \&#34;5\&#34;-&gt;\&#34;1\&#34; \n  \&#34;6\&#34;-&gt;\&#34;4\&#34; \n  \&#34;7\&#34;-&gt;\&#34;2\&#34; \n  \&#34;8\&#34;-&gt;\&#34;3\&#34; \n  \&#34;9\&#34;-&gt;\&#34;4\&#34; \n  \&#34;2\&#34;-&gt;\&#34;10\&#34; \n  \&#34;3\&#34;-&gt;\&#34;11\&#34; \n  \&#34;3\&#34;-&gt;\&#34;12\&#34; \n  \&#34;4\&#34;-&gt;\&#34;12\&#34; \n  \&#34;4\&#34;-&gt;\&#34;13\&#34; \n  \&#34;14\&#34;-&gt;\&#34;5\&#34; \n  \&#34;15\&#34;-&gt;\&#34;5\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;The figure implies that motion on the retina comes from the movement of objects in the environment, movement of the observer, specifically the eyes, head, or body, or a combination of any of these. Eye movements stem from eye movement commands; head movements (translations and rotations) stem from head movement commands; body movements stem from commands to move the body. Movements of the eyes, head, or body generate proprioceptive signals from the eyes, head, and body. These signals derive from muscle, joint, and tendon receptors. Rotational and translational motion of the head stimulates vestibular signals.&lt;/p&gt;
&lt;p&gt;Note that retinal motion caused by the observer has multiple non-visual correlates (or consequences) that are detectable under typical circumstances. There may also be differences in the specific &lt;em&gt;properties&lt;/em&gt; of motion caused by eye, head, body, or object motion. We’ll discuss these in a future post.&lt;/p&gt;
&lt;p&gt;So, the task for perception is to answer the question: What’s moving and why?&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>open science | Rick Gilmore&#39;s site</title>
    <link>/category/open-science/</link>
      <atom:link href="/category/open-science/index.xml" rel="self" type="application/rss+xml" />
    <description>open science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2018-2020</copyright><lastBuildDate>Tue, 04 Jun 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>open science</title>
      <link>/category/open-science/</link>
    </image>
    
    <item>
      <title>Is there a crisis of reproducibility in science?</title>
      <link>/post/is-there-a-crisis-of-reproducibility-in-science/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/is-there-a-crisis-of-reproducibility-in-science/</guid>
      <description>&lt;p&gt;A recent 
&lt;a href=&#34;https://doi.org/doi/10.1073/pnas.1806370115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt; in the &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; by Redish et al. argues that &amp;lsquo;reproducibility failures are essential to scientific inquiry.&amp;rsquo;
The authors remind readers that progress in many fields often proceeds haltingly, with successes, retrenchments, reconsiderations, and revisions.
As examples, Redish et al. summarize the history of the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Four_color_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Four Color Theorem&lt;/a&gt;, the claim that 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fourier series&lt;/a&gt; can characterize any function, and the conditions under which neural networks can carry out certain computations.
I find useful these reminders from mathematics and computer science that progress and regress are essential parts of the trajectory of scientific discovery, but I find unconvincing arguments that &amp;lsquo;many of the current concerns about reproducibility overlook the dynamic, iterative nature of the process of discovery.&amp;rsquo;&lt;/p&gt;
&lt;p&gt;According to a 
&lt;a href=&#34;https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2016 survey in Nature&lt;/a&gt;, some 90% of scientists surveyed agreed that &amp;lsquo;there is a reproducibility crisis.&amp;rsquo;
Sixty to eighty percent of scientists in fields from chemistry to earth and environmental sciences reported failing to reproduce someone else&amp;rsquo;s experiment, and 40-60% reported failing to reproduce their own experiments.
Respondents expressed widely-ranging opinions about &amp;lsquo;how much published work in your field is reproducible?&amp;rsquo;
Curiously, physicists and chemists were most confident in the literature AND most likely to report having failed to reproduce a study.
Perhaps chemists and physicists are more likely to attempt to reproduce published results than scientists in other fields.&lt;/p&gt;
&lt;p&gt;Redish et al. rightly point out that no single study should be viewed as definitive, and that discovering robust findings takes time.
But I think the authors miss an opportunity to talk about what specific practices accelerate the process of discovery and what practices retard it.
Open sharing of results, positive and negative, data, experimental procedures, analysis code, and materials accelerates progress.
In the fields of psychology, vision science, and neuroscience I am most familiar with, failures to replicate are almost impossible to publish.
If the failures aren&amp;rsquo;t published, discovering the reasons for the failure will be difficult to discern.
Open data, materials, analysis code, and procedure sharing are slowly growing in popularity, but still not commonplace.
Without ready access to these elements, it is almost certain that attempts to replicate will differ from the original finding in ways that may or may not be readily apparent.&lt;/p&gt;
&lt;p&gt;Redish et al. state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A failure to reproduce is only the first step in scientific inquiry. In many ways, how science responds to these failures is what determines whether it succeeds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I agree.
And the response of many open science advocates is to encourage our colleagues to adopt more transparent practices in reporting and sharing results, data, analyses, and materials, and to build 
&lt;a href=&#34;https://databrary.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tools&lt;/a&gt; for making this easy.
If we want to build &amp;lsquo;a genuinely cumulative science&amp;rsquo;, to borrow a phrase from 
&lt;a href=&#34;https://www.psychologicalscience.org/observer/becoming-a-cumulative-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Walter Mischel&lt;/a&gt;, we need to attend to improving the processes of self-correction that Redish et al. rightly point out are endemic to science and essential to its power.
Right now, it&amp;rsquo;s far too difficult for researchers to learn about failures to replicate or generalize and far too difficult to explore the reasons why.
So perhaps the crisis of reproducibility is more usefully thought of as a crisis of transparency, and if this is so, there are ready solutions at hand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Devilish details</title>
      <link>/post/devilish-details/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/post/devilish-details/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve found myself at odds recently with colleagues who, like me, view themselves as advocates for open science.
This post attempts to clarify what I see as the crux of our disagreements.&lt;/p&gt;
&lt;h2 id=&#34;we-dont-own-our-data&#34;&gt;We don&amp;rsquo;t own our data&lt;/h2&gt;
&lt;p&gt;Most researchers feel a strong sense of ownership about our data and often, properly so I think, feel especially protective of our research participants.
The fact remains, however, that despite our essential role in determining what data get collected from whom and how, we researchers have the weakest claim to actually &lt;em&gt;owning&lt;/em&gt; the data we collect.&lt;/p&gt;
&lt;p&gt;In the U.S., if my work is funded by a grant from the Federal government (e.g., NSF or NIH), then that award is actually made to &lt;em&gt;my institution&lt;/em&gt;, not me.
In fact, if I am unable to fulfill the obligations of the award, my institution may, and often will, appoint another person to do so.
If my work is funded by another source, that source is likely to have implemented similar provisions.
So, in reality, while researchers have a primary stewardship role over research data, it likely belongs to our institution.&lt;/p&gt;
&lt;p&gt;In other contexts, the EU, for example, strong arguments have been made that &lt;em&gt;research participants&lt;/em&gt; actually own their data.
I am sympathetic to these arguments and believe that empowering individuals to grant researchers specific permission to use their data is the best path toward a future of ethical data use.
Implementing that vision at-scale poses substantial challenges I believe academic researchers can eventually meet.
The point of convergence is that researchers don&amp;rsquo;t own the data we collect.&lt;/p&gt;
&lt;p&gt;Not owning the data nevertheless means that we researchers must still think carefully about how best to protect it, our participants, and our institutions.
As a practical matter, even though our institutions (or research participants) may own the data, we are the ones best positioned to determine what happens to those data when our project ends.
In other words, the responsibility for determining the future life, if any, of our research data is one we can&amp;rsquo;t meaningfully shirk or easily ignore.
At the same time, because the data aren&amp;rsquo;t really ours,  considerations about any personal or career risk associated with sharing data can&amp;rsquo;t be at the center of the discussion.
Instead, our obligations to research participants, our institutions, and funding agencies must take precedence.
This means that fear of being scooped probably isn&amp;rsquo;t sufficient justification to withhold sharing data.&lt;/p&gt;
&lt;h2 id=&#34;sharing-data-publicly-without-restriction-is-unnecessary-and-poses-unforseen-risks&#34;&gt;Sharing data publicly without restriction is unnecessary and poses unforseen risks&lt;/h2&gt;
&lt;p&gt;I believe that some of my colleagues who are skeptical about data sharing understand the practice to mean sharing data in publicly accessible ways.
These skeptics argue that they don&amp;rsquo;t have permission to share data that way, and thus doing so would violate research ethics.
I agree.&lt;/p&gt;
&lt;p&gt;In fact, I go further.
I am worried that we woefully underestimate the risks of participant reidentifiability associated with combining public and private data sets, even when research data have been stripped of conventional identifiers.
Imagine if Bernie Sanders and Donald Trump had taken intro psych classes and the &amp;lsquo;deidentified&amp;rsquo; data were publicly available to anyone today.
Doesn&amp;rsquo;t it seem likely, certain even, that large well-funded teams of data scientists would be scouring the internet in order to try to expose Bernie&amp;rsquo;s answer to this survey question or Donald&amp;rsquo;s score on that cognitive test?&lt;/p&gt;
&lt;p&gt;We can&amp;rsquo;t predict the future, but we can try to shape it.&lt;/p&gt;
&lt;p&gt;Rather than make open, public sharing of data about human beings the expected norm, I suggest we build on an existing track-record of sharing these sorts data with restricted audiences.
The Inter-university Consortium on Political and Social Research (
&lt;a href=&#34;https://icpsr.umich.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICSPR&lt;/a&gt;) at the University of Michigan has been storing and sharing data with restricted audiences for almost 60 years.
The U.S. Census has developed procedures for allowing restricted access to individual Census survey responses in highly secure and protected environments.
The 
&lt;a href=&#34;https://databrary.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Databrary&lt;/a&gt; data library that I co-founded and co-direct has created a data library for openly sharing video and audio recordings with a restricted community of researchers.
There are numerous examples of data sets shared by the U.S. government (e.g., NDAR) that have various restrictions on who can access data and for what purposes.&lt;/p&gt;
&lt;p&gt;These examples illustrate that open data sharing need not be synonymous with public data sharing.
The ICPSR and Census examples especially show that even the most sensitive data can be successfully shared with responsible researchers.&lt;/p&gt;
&lt;p&gt;Yes, restricted sharing with researchers affiliated with institutions of higher learning or academic medical centers is &amp;lsquo;elitist&amp;rsquo; to some degree.
But the checks and balances these institutions provide &amp;ndash; research ethics training, research review committees, research supervision, secure data storage infrastructure &amp;ndash; protect research participants in ways that public data sharing schemes simply cannot, at least not now.
I think this form of elitism is justified in the name of protecting research participants.
Or at the very least, that restricting data about human participants should be the norm unless and until we can be highly confident that public sharing truly poses minimal risk.&lt;/p&gt;
&lt;p&gt;Thus, I conclude the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Virtually all data about human research participants can be shared in some form.&lt;/li&gt;
&lt;li&gt;The standard for sharing data about human research participants should be to use a data repository that restricts access to specific individuals for purposes subject to external review.&lt;/li&gt;
&lt;li&gt;All researchers who collect data about human research participants should be collecting permission to share information from their participants so that any data sharing activities are in accord with participants&amp;rsquo; wishes. There are readily available 
&lt;a href=&#34;https://www.databrary.org/resources/templates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;templates&lt;/a&gt; researchers can adapt for seeking sharing permission. Going forward there is no reason for &amp;lsquo;failure to secure permission to share&amp;rsquo; to be a meaningful impediment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, some may argue that the cost of securing permission to share and storing data in restricted repositories versus publicly accessible places must be weighed against the benefits.
I don&amp;rsquo;t disagree that costs and benefits must be weighed, but it is often hard to weigh them objectively.
Instead, I think it is safer to say that we have a longer history of and familiarity with restricted data sharing and more information about the costs of preparing data to share in these sorts of settings.
So the surer bet in the long-term is to build on what has already been working.
The risks of widespread public data sharing are a &amp;lsquo;known-unknown&amp;rsquo;, in Donald Rumsfeld&amp;rsquo;s categorical scheme, while the risks of restricted data sharing are &amp;lsquo;known-knowns&amp;rsquo;.
I do not share Mr. Rumsfeld&amp;rsquo;s politics, but I find his thinking incisive and useful  in this context.&lt;/p&gt;
&lt;p&gt;One final point in favor of restricted sharing in centralized repositories is metadata standardization.
If we view data sharing as an investment in future scholarship, then sharing data in standard ways that others can combine, reuse, and build upon is essential.
By design (e.g., Databrary, government-sponsored data repositories) or long-standing practice (ICSPSR), data repositories provide standard metadata elements that make data more readily discoverable and reusable.
Of course, curating data to meet these metadata standards takes time, but the payoff comes in accelerated resuse and discovery.&lt;/p&gt;
&lt;p&gt;So, data about human participants should be shared (with permission) in data repositories that restrict access and support standardized metadata, and all of us who do research with human participants should be seeking explicit permission to share in these ways.&lt;/p&gt;
&lt;h2 id=&#34;sharing-data-materials-and-analysis-scripts-is-more-important-to-open-science-than-pre-registration&#34;&gt;Sharing data, materials, and analysis scripts is more important to open science than pre-registration&lt;/h2&gt;
&lt;p&gt;I greatly admire the efforts of the team at the 
&lt;a href=&#34;https://cos.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Center for Open Science&lt;/a&gt; in bringing preregistration and registered reports into widespread scientific discussion and making them a much more common practice.
There is no question that drawing clearer, brighter lines between confirmatory and exploratory analyses improves the quality of scientific inference and mitigates questionable research practices.&lt;/p&gt;
&lt;p&gt;That said, I get the biggest and hardest push-back from colleagues about preregistration when discussing open science practices.
One reason is that many people find the process of pregistration difficult, frustrating, and onerous.
Yes, there are lots of templates to choose from, and yes, many of the details are already in our IRB protocols or grant proposals.
But even so, the benefits of pre-registration seem small relative to the high costs, especially to skeptics.&lt;/p&gt;
&lt;p&gt;For these reasons, and innate pragmatism, I spend more of my energy talking-up data sharing.
Data are more interesting to me personally and probably more useful in evaluating the impact and importance of a given piece of work than a detailed preregistration plan.
One of my graduate school mentors told me that he reads a paper&amp;rsquo;s methods and results first, and only after that decides whether to read the introduction and conclusion.
Frankly, if the data aren&amp;rsquo;t interesting or the effects clear, I don&amp;rsquo;t care that much what the authors predicted or how they tortured the data.
And by extension, if the effect is strong, it&amp;rsquo;s unlikely to matter much whether the authors predicted it or not.
Further, I&amp;rsquo;ve found no convincing retort to the skeptics who ask me why I think someone actually followed their preregistered procedure.
In short, I think that most studies on human participants are under-powered and that we need to vastly increase the quantity of shared data in order to address problems of estimating true effect size distributions.
While I may not really know whether a researcher followed the protocol reported in a manuscript or in their preregistration, I can evaluate the size of the reported effect and hopefully combine it with other datasets&amp;hellip;if the data are shared.
When push comes to shove, I&amp;rsquo;d rather authors commit to open data sharing and carry through on their commitments than encourage people to preregister but have them stop short of sharing.&lt;/p&gt;
&lt;p&gt;So, while I&amp;rsquo;m completely happy endorsing the virtues of preregistration, I&amp;rsquo;m going to keep asking authors of papers I review, most of which are not preregistered, to share their data.
That&amp;rsquo;s because I think data, materials, and analysis code sharing is ultimately more important to advancing discovery than preregistration.
And &amp;lsquo;learn more faster&amp;rsquo; is not only Databrary&amp;rsquo;s tagline, is my personal motivation for adopting open science practices.
That ultimate goal can get lost in the sometimes devilish discussions about specific details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open developmental science</title>
      <link>/post/open-developmental-science/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/open-developmental-science/</guid>
      <description>&lt;p&gt;If you plan to attend the Society for Research in Child Development (SRCD) meeting in Baltimore later this week, you might be interested in some of the open science activities that will be taking place:&lt;/p&gt;
&lt;p&gt;On Friday, March 22, from 10-11:30 am, I am co-leading a conversation hour on the topic 
&lt;a href=&#34;https://www.rick-gilmore.com/talk/srcd-2019-03-22/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;What SRCD is doing to address open science&lt;/em&gt;&lt;/a&gt;.
We&amp;rsquo;ll focus on a set of new policies and guidelines to authors that the SRCD Task Force on Scientific Integrity and Openness produced.&lt;/p&gt;
&lt;p&gt;In addition, there are other open science events happening at SRCD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;ManyBabies: Bringing large-scale collaborative projects to infant development research&lt;/em&gt;, Saturday, March 23, 12:45-2:15 pm, Hilton Baltimore, Level 2, Key 4&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Making developmental science more open: Successes, obstacles, and solutions&lt;/em&gt;, Saturday, March 23, 2:30-4:00 pm, Baltimore Convention Center, Room 309&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See you in Baltimore!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rick Gilmore&#39;s site on Rick Gilmore&#39;s site</title>
    <link>/</link>
    <description>Recent content in Rick Gilmore&#39;s site on Rick Gilmore&#39;s site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>In search of the ethogram</title>
      <link>/post/in-search-of-the-ethogram/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/in-search-of-the-ethogram/</guid>
      <description>&lt;p&gt;Psychology is in part, a science of behavior.
Mental experience &amp;ndash; thoughts and feelings &amp;ndash; are also common targets of inquiry.&lt;/p&gt;

&lt;p&gt;I recently went looking for a systematic catalogue of behaviors that should, in theory, constitute the substance of what psychologists and other behavioral scientists study.
My goal was to take that catalogue as a starting point for thinking about the relationship between information available for perception and the actions (behaviors) that this information informs.
The formal term for this sort of catalog is an &amp;lsquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Ethogram&#34; target=&#34;_blank&#34;&gt;ethogram&lt;/a&gt;&amp;rsquo;.
Here&amp;rsquo;s a sample &lt;a href=&#34;http://tolweb.org/onlinecontributors/app?page=TeacherResourceViewSupportMaterial&amp;amp;service=external&amp;amp;sp=l3090&amp;amp;sp=4&#34; target=&#34;_blank&#34;&gt;template&lt;/a&gt; from the Tree of Life project.
Unfortunately, I failed in my quest.&lt;/p&gt;

&lt;p&gt;There have been attempts to create and openly share animal ethograms like the apparently stalled or inactive ethobank.org site associated with the EthoSource project.
The &lt;a href=&#34;http://www.cogpo.org/&#34; target=&#34;_blank&#34;&gt;Cognitive Paradigm Ontology&lt;/a&gt; and the &lt;a href=&#34;http://www.cognitiveatlas.org/&#34; target=&#34;_blank&#34;&gt;Cognitive Atlas&lt;/a&gt; project attempt to create systematic ontologies (systematic representations of categories and concepts and their relations) of tasks used in experimental psychology.
Cognitive psychology tasks clearly require very specific types of behavior from participants.
Systematically organizing the nature of these tasks should lead to faster progress in understanding how they relate to one another, and ultimately, how minds and brains actually work.
Still, I came up short in finding a comprehensive human ethogram.&lt;/p&gt;

&lt;p&gt;In other contexts, I&amp;rsquo;ve called such a thing the behaviorome, partly to suggest that behavior should have a more central place in research grantmaking priorities.
The &lt;a href=&#34;/project/PLAY/&#34;&gt;PLAY project&lt;/a&gt; is a type of behaviorome effort.
We will be creating detailed definitions for specific behaviors that our infant and adult participants produce in a small set of behavioral domains, but it will not be comprehensive by any means.
For now, I&amp;rsquo;m stuck with trying to assemble my own.&lt;/p&gt;

&lt;p&gt;One approach to an ethogram for humans could focus on biologically essential behavioral classes that we share with all other animals: &lt;em&gt;ingestion&lt;/em&gt; (food and nutrient seeking), &lt;em&gt;defense&lt;/em&gt; (self-protection), and &lt;em&gt;reproduction&lt;/em&gt;.
Unfortunately, this scheme fail to capture the sets of behaviors studied by the majority of psychologists.
Adding in a class of communicative and affiliative behaviors helps.
But even this augmented classification describes the functional purposes of behavior, not the behaviors themselves.
For example, locomotor behavior, moving toward or away from some target, occurs in food seeking, predator avoidance, mate seeking, and social (affiliative) contexts.&lt;/p&gt;

&lt;p&gt;Another way to ask the question is to observe whether physical activities that humans do for pleasure and watch others for entertainment (sports) have some latent structure we might exploit.
Here, we assume that if large groups of humans engage in an activity or pay to watch others engage in it, the skills involved must be especially demanding.
Indeed, professional sports represent some of humankind&amp;rsquo;s most skilled (and highly paid) actors.
And while I&amp;rsquo;m still looking for a comprehensive catalog of sport types, this table shows that most of them fall into a small set of categories when focusing on the core actions that define success.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Sports&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;aiming&lt;/td&gt;
&lt;td&gt;basketball, baseball, football (passing &amp;amp; kicking), hocky, soccer, bowling, tennis, golf, pool/billiards, archery, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;steering&lt;/td&gt;
&lt;td&gt;slalom skiing, motorsports&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;performing&lt;/td&gt;
&lt;td&gt;gymnastics, diving, synchronized swimming, figure skating, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;racing&lt;/td&gt;
&lt;td&gt;running, motorsports, speed skating, swimming, cycling, paddling&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;fighting&lt;/td&gt;
&lt;td&gt;boxing, wrestling, &amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feats of strength&lt;/td&gt;
&lt;td&gt;weightlifting&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You may disagree with my categorization, but I hope you&amp;rsquo;ll agree that there are many similarities in the sorts of actions we find entertaining to watch.&lt;/p&gt;

&lt;p&gt;Another approach to an ethogram starts with the actions that computer vision researchers find are important to extract from video.
Google’s Atomic Visual Action (AVA) dataset (&lt;a href=&#34;https://research.google.com/ava/&#34; target=&#34;_blank&#34;&gt;https://research.google.com/ava/&lt;/a&gt;) involves three classes of movements, &lt;em&gt;person movement&lt;/em&gt;, &lt;em&gt;object interaction&lt;/em&gt;, &lt;em&gt;person interaction&lt;/em&gt;, and has exemplars with 81 specific actions that fit into these categories.
Some examples of specific behaviors tagged in the AVA dataset are &lt;em&gt;crawl&lt;/em&gt;, &lt;em&gt;dance&lt;/em&gt;, &lt;em&gt;jump/leap&lt;/em&gt;, &lt;em&gt;lift&lt;/em&gt;, &lt;em&gt;open&lt;/em&gt;, &lt;em&gt;press&lt;/em&gt;, &lt;em&gt;hug&lt;/em&gt;, &lt;em&gt;kiss&lt;/em&gt;, and &lt;em&gt;listen to&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Maybe some combination of these approaches is as good a start as any.
After all, if &lt;a href=&#34;http://doi.org/10.1016/j.anbehav.2009.03.018&#34; target=&#34;_blank&#34;&gt;behavioral biologists don&amp;rsquo;t seem to agree about what constitutes behavior&lt;/a&gt;, it&amp;rsquo;s not that surprising that psychologists haven&amp;rsquo;t yet created our own list.
We do need a list like this, though, don&amp;rsquo;t you think?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parameters for action</title>
      <link>/post/parameters-for-action/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/parameters-for-action/</guid>
      <description>


&lt;p&gt;Several years ago, Florian Raudies, Swapnaa Jayaraman and I published a &lt;a href=&#34;http://doi.org/10.1109/DEVLRN.2015.7345450&#34;&gt;paper&lt;/a&gt; where we simulated the optic flow that infants would experience in different head/body postures. We computed cyclopian (one-eyed) flow on the basis of this schematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gilmore-lab/temple-2017-02-27/master/img/computing-flow.jpg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Here, the key parameters were the instantaneous translation &lt;span class=&#34;math inline&#34;&gt;\((v_x{}, v_y{}, v_z{})\)&lt;/span&gt; and rotation &lt;span class=&#34;math inline&#34;&gt;\((\omega_{x}, \omega_y{}, \omega_z{})\)&lt;/span&gt; of the planar retina. Coupled with the optic flow equation,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\begin{pmatrix}\dot{x} \\ \dot{y}\end{pmatrix}=\frac{1}{z} \begin{pmatrix}-f &amp;amp; 0 &amp;amp; x\\ 0 &amp;amp; -f &amp;amp; y \end{pmatrix} \begin{pmatrix}{v_x{}}\\ {v_y{}} \\{v_z{}}\end{pmatrix}+ \frac{1}{f} \begin{pmatrix} xy &amp;amp; -(f^2+x^2) &amp;amp; fy\\ f^2+y^2 &amp;amp; -xy &amp;amp; -fy \end{pmatrix} \begin{pmatrix} \omega_{x}\\ \omega_{y}\\ \omega_{z} \end{pmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we were able to simulate the &lt;em&gt;perceptual&lt;/em&gt; effects of postural geometry: Changes in eye height and forward translational speed that would occur when a child changed from crawling to walking altered the pattern of retinal flow &lt;span class=&#34;math inline&#34;&gt;\((\dot{x}, \dot{y})\)&lt;/span&gt; in interesting ways.&lt;/p&gt;
&lt;p&gt;This work has lain dormant for a few years, but I now want to pick it back up. In short, there are a handful of perception/action systems that provide the nervous system with deterministic, causal information about the effects of different actions. These must be important for development.&lt;/p&gt;
&lt;p&gt;For the next step, I’m looking for a concise, but thorough parameterization of body posture that includes the eyes, head, torso, arms, and legs. Here’s a sketch of what I have in mind for the upper parts body that have the greatest impact on the direction of visual fixation:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Body part&lt;/th&gt;
&lt;th&gt;Parameter(s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Eyes&lt;/td&gt;
&lt;td&gt;$&lt;em&gt;{rx}, &lt;/em&gt;{ry}, &lt;em&gt;{lx}, &lt;/em&gt;{lx} $&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Head&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_x{}, \theta_y{}, \theta_z{}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Torso&lt;/td&gt;
&lt;td&gt;$_x{}, _y{}, _z{} $&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Coupled with the distance between the eyes, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the radial distance to the head’s center of rotation, &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, and the distance from the head’s center of rotation to the torso’s center of rotation, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, we can compute the effects of eye, head, and torso movement on visual motion at the two retinae. Now, if the &lt;em&gt;visual&lt;/em&gt; signals from eye vs. head vs. torso can be distinguished, then these could couple with other proprioceptive (muscle, tendon, cutaneous) signals to provide a powerful set of &lt;em&gt;sensory&lt;/em&gt; signals that are directly caused by eye, head, and torso motion. See &lt;a href=&#34;../the-webs-we-weave/&#34;&gt;this earlier post&lt;/a&gt; for a causal graph that elaborates on this point. I’ll discuss why I think there are &lt;em&gt;visual&lt;/em&gt; differences in the effects of eye and head motion in a future post.&lt;/p&gt;
&lt;p&gt;My next step is to ask my colleagues in kinesiology if there is a canonical parameterization of body position that I can build upon. If you know of one, let me know.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The webs we weave</title>
      <link>/post/the-webs-we-weave/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-webs-we-weave/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve been ‘sketching’ out conceptual models for some time without realizing that there is an entire field of analysis where these sorts of diagrams are the starting point: Causal modeling. My introduction to this world was Judea Pearl’s &lt;a href=&#34;https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1548436587&amp;amp;sr=8-1&amp;amp;keywords=the+book+of+why&#34;&gt;The Book of Why: The New Science of Cause and Effect&lt;/a&gt;. Interested casual readers might also enjoy Steve Sloman’s excellent and delightfully clear &lt;a href=&#34;https://www.amazon.com/Causal-Models-People-Think-Alternatives/dp/0195394291/ref=sr_1_2?ie=UTF8&amp;amp;qid=1548436730&amp;amp;sr=8-2&amp;amp;keywords=steven+sloman&#34;&gt;&lt;em&gt;Causal Models: How People Think About the World and Its Alternatives&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I want to build a model for how an observer perceives visual motion given various possible causes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DiagrammeR)

nodes &amp;lt;- create_node_df(
  n = 15,
  label = c(&amp;quot;retinal motion&amp;quot;, &amp;quot;eye mvmt&amp;quot;, &amp;quot;head mvmt&amp;quot;, &amp;quot;body mvmt&amp;quot;, &amp;quot;object mvmt&amp;quot;, &amp;quot;passive transp&amp;quot;, &amp;quot;eye cmd&amp;quot;, &amp;quot;head cmd&amp;quot;, &amp;quot;body cmd&amp;quot;, &amp;quot;eye prop&amp;quot;, &amp;quot;head prop&amp;quot;, &amp;quot;head vest&amp;quot;, &amp;quot;body prop&amp;quot;, &amp;quot;entity&amp;quot;, &amp;quot;passive force&amp;quot;),
  type = &amp;quot;number&amp;quot;
)

edges &amp;lt;- create_edge_df(
  from = c(2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 3, 4, 4, 14, 15),
  to = c(1, 1, 1, 1, 4, 2, 3, 4, 10, 11, 12, 12, 13, 5, 5),
  rel = &amp;quot;related&amp;quot;
)

graph &amp;lt;- DiagrammeR::create_graph(nodes_df = nodes, 
                                  edges_df = edges)

graph %&amp;gt;% DiagrammeR::render_graph()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\ngraph [layout = \&#34;neato\&#34;,\n       outputorder = \&#34;edgesfirst\&#34;,\n       bgcolor = \&#34;white\&#34;]\n\nnode [fontname = \&#34;Helvetica\&#34;,\n      fontsize = \&#34;10\&#34;,\n      shape = \&#34;circle\&#34;,\n      fixedsize = \&#34;true\&#34;,\n      width = \&#34;0.5\&#34;,\n      style = \&#34;filled\&#34;,\n      fillcolor = \&#34;aliceblue\&#34;,\n      color = \&#34;gray70\&#34;,\n      fontcolor = \&#34;gray50\&#34;]\n\nedge [fontname = \&#34;Helvetica\&#34;,\n     fontsize = \&#34;8\&#34;,\n     len = \&#34;1.5\&#34;,\n     color = \&#34;gray80\&#34;,\n     arrowsize = \&#34;0.5\&#34;]\n\n  \&#34;1\&#34; [label = \&#34;retinal motion\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34; [label = \&#34;eye mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;3\&#34; [label = \&#34;head mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;4\&#34; [label = \&#34;body mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;5\&#34; [label = \&#34;object mvmt\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;6\&#34; [label = \&#34;passive transp\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;7\&#34; [label = \&#34;eye cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;8\&#34; [label = \&#34;head cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;9\&#34; [label = \&#34;body cmd\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;10\&#34; [label = \&#34;eye prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;11\&#34; [label = \&#34;head prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;12\&#34; [label = \&#34;head vest\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;13\&#34; [label = \&#34;body prop\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;14\&#34; [label = \&#34;entity\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;15\&#34; [label = \&#34;passive force\&#34;, fillcolor = \&#34;#F0F8FF\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34;-&gt;\&#34;1\&#34; \n  \&#34;3\&#34;-&gt;\&#34;1\&#34; \n  \&#34;4\&#34;-&gt;\&#34;1\&#34; \n  \&#34;5\&#34;-&gt;\&#34;1\&#34; \n  \&#34;6\&#34;-&gt;\&#34;4\&#34; \n  \&#34;7\&#34;-&gt;\&#34;2\&#34; \n  \&#34;8\&#34;-&gt;\&#34;3\&#34; \n  \&#34;9\&#34;-&gt;\&#34;4\&#34; \n  \&#34;2\&#34;-&gt;\&#34;10\&#34; \n  \&#34;3\&#34;-&gt;\&#34;11\&#34; \n  \&#34;3\&#34;-&gt;\&#34;12\&#34; \n  \&#34;4\&#34;-&gt;\&#34;12\&#34; \n  \&#34;4\&#34;-&gt;\&#34;13\&#34; \n  \&#34;14\&#34;-&gt;\&#34;5\&#34; \n  \&#34;15\&#34;-&gt;\&#34;5\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;The figure implies that motion on the retina comes from the movement of objects in the environment, movement of the observer, specifically the eyes, head, or body, or a combination of any of these. Eye movements stem from eye movement commands; head movements (translations and rotations) stem from head movement commands; body movements stem from commands to move the body. Movements of the eyes, head, or body generate proprioceptive signals from the eyes, head, and body. These signals derive from muscle, joint, and tendon receptors. Rotational and translational motion of the head stimulates vestibular signals.&lt;/p&gt;
&lt;p&gt;Note that retinal motion caused by the observer has multiple non-visual correlates (or consequences) that are detectable under typical circumstances. There may also be differences in the specific &lt;em&gt;properties&lt;/em&gt; of motion caused by eye, head, body, or object motion. We’ll discuss these in a future post.&lt;/p&gt;
&lt;p&gt;So, the task for perception is to answer the question: What’s moving and why?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>QSL? QSL</title>
      <link>/post/qsl-qsl/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/qsl-qsl/</guid>
      <description>&lt;p&gt;Last night, my friend Mike (N3LI) and I made our first-ever CW QSOs.
In ham-speak, that means we chatted using Morse code.&lt;/p&gt;

&lt;p&gt;It wasn&amp;rsquo;t fast, and we didn&amp;rsquo;t copy each other completely.
But we exchanged meaningful information using a classic communication mode that is no longer a requirement for licensing but is still going strong.
It was great fun, and we plan to keep practicing and plan to meet-up again next week.&lt;/p&gt;

&lt;p&gt;Re-learning Morse code has reinforced several things for me.
Patience and persistence are the most important qualities for long-term mastery of any difficult skill.
And it&amp;rsquo;s good to give older brains a tough, but realizable challenge.
Especially when there are concrete and visible measures of success.&lt;/p&gt;

&lt;p&gt;So any no-code hams out there who haven&amp;rsquo;t taken the plunge, come on in!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New lab logo</title>
      <link>/post/new-lab-logo/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/new-lab-logo/</guid>
      <description>&lt;p&gt;For I don&amp;rsquo;t remember how many years I&amp;rsquo;ve wanted a lab logo.
I thought that designing a logo was something that &lt;em&gt;some&lt;/em&gt; student might actually enjoy doing.
But I never pushed or insisted, and so it never happened.&lt;/p&gt;

&lt;p&gt;Then, as a byproduct of a bunch of reading I&amp;rsquo;ve been doing about causal modeling and the philosophy of science, I came up with a figure that seems to work on several levels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../img/nested-causality.jpg.png&#34; height=150px/&gt;&lt;/p&gt;

&lt;p&gt;The outer circle represents the world, the next circle the body that is embedded in the world, the next the nervous system embedded in that body (in the world), and the smallest the mind embedded in the brain (in the body in the world).
The world, body, and brain are directly measurable in various ways.
The mind is not.
We infer properties and structures of the mind indirectly, through patterns of brain activity or peripheral physiology or behavior.
The focal point of psychology as a science is making inferences about something that cannot be directly measured.
This is one reason psychology is harder than physics.&lt;/p&gt;

&lt;p&gt;The figure has other associations.
It is similar to one used to demonstrate that rotational motion in 2 dimensions can evoke the perception of an object in 3 dimensions &amp;ndash; something called the stereokinetic depth effect (SKE).
It evokes the perception of a cone-like shape protruding up and to the left.
And it evokes a weaker perception of a tunnel.
I study depth perception and motion perception, so these features make the figure even more appealing.&lt;/p&gt;

&lt;p&gt;Finally, the figure could be viewed as a large letter O, for openness, something I now embrace in all aspects of my scholarship.&lt;/p&gt;

&lt;p&gt;I confess that I was just trying a new way to depict the embedded causal relationships among mind, brain, body, and the environment and found a color scheme I liked.
The rest was purely accidental.
But I guess invention is like that sometimes.&lt;/p&gt;

&lt;p&gt;I hope you like it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building walls and tearing them down</title>
      <link>/post/building-walls-and-tearing-them-down/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/building-walls-and-tearing-them-down/</guid>
      <description>&lt;p&gt;The building of walls is of much concern these days.
This song from Anais Mitchell&amp;rsquo;s folk opera &amp;ldquo;Hadestown&amp;rdquo; sheds light on &lt;em&gt;why&lt;/em&gt; some of us want so desperately to build them:&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/8sQ8R54C53o&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;I must also wonder what future Mexican leader will call on us to tear down the wall we seem so desperate to build?&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/GCO9BYCGNeY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Walls are no symbol of strength.
They shut in more than they keep out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PLAY project</title>
      <link>/project/play/</link>
      <pubDate>Fri, 28 Dec 2018 16:55:00 -0500</pubDate>
      
      <guid>/project/play/</guid>
      <description>&lt;p&gt;The Play &amp;amp; Learning Across a Year (PLAY) project will provide a first-ever glimpse inside the homes of almost 1,000 families across the U.S.
We will collect, annotate (code), and openly share on &lt;a href=&#34;https://databrary.org&#34; target=&#34;_blank&#34;&gt;Databrary&lt;/a&gt; video recordings of mothers and 12-, 18-, and 24-month-old infants engaged in natural behavior in their homes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test-driven development is hard...and important.</title>
      <link>/post/test-driven-development-is-hard-and-important/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/test-driven-development-is-hard-and-important/</guid>
      <description>


&lt;p&gt;I’ve been developing an R package that interacts with the &lt;a href=&#34;https://databrary.org&#34;&gt;Databrary.org&lt;/a&gt; API and with &lt;a href=&#34;https://datavyu.org&#34;&gt;Datavyu&lt;/a&gt; annotation files stored locally or on Databrary alongside shared videos. If you’re curious, you can download the &lt;code&gt;databraryapi&lt;/code&gt; package from this GitHub repository: &lt;a href=&#34;https://github.com/PLAY-behaviorome/databraryapi&#34; class=&#34;uri&#34;&gt;https://github.com/PLAY-behaviorome/databraryapi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Like many people in the software world, I’m entirely self-taught. Ok, I took a class in C programming at the U.S. Department of Agriculture’s Graduate School in the year before I applied to graduate schools in cognitive neuroscience. But my R package development skills are entirely self-taught. I must thank the incredibly generous geniuses who have gone before me and who share their code and their talents so freely and openly. Without the almost instant availability of these resources, my progress would be much, much slower.&lt;/p&gt;
&lt;p&gt;Developers are an opinionated bunch, and there are at least as many styles (fads?) in software development as their are developers. One style that I have started to try to emulate is &lt;a href=&#34;https://en.wikipedia.org/wiki/Test-driven_development&#34;&gt;‘test-driven development’&lt;/a&gt;. In TDD, the idea is that you create tests for how each part of your package should respond given this or that input. If your tests are through enough and correct, your package should work…at least within the boundaries of what you tested.&lt;/p&gt;
&lt;p&gt;For the latest version of the package (0.1.4), I added a bunch of new tests to evaluate several new functions I’ve added to the package. Let’s just say that getting through my own self-designed test battery was challenging. But as a result, the code is cleaner and less buggy than it would be if I hadn’t gone this route.&lt;/p&gt;
&lt;p&gt;In the larger sense, TDD is sort of a “plan for the worst” style. I like it because I know it forces me to be more precise and specific than I might otherwise choose to be. Since I’ve been using the &lt;code&gt;databraryapi&lt;/code&gt; more often for keeping tabs on what’s going on in the Databrary world, that’s a very good thing.&lt;/p&gt;
&lt;p&gt;In case you’re curious what the package can do, check this out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;PLAY-behaviorome/databraryapi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skipping install of &amp;#39;databraryapi&amp;#39; from a github remote, the SHA1 (a6a700ea) has not changed since last install.
##   Use `force = TRUE` to force installation&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a list of some recently authorized researchers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;databraryapi::get_db_stats(&amp;quot;people&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##      id sortname  prename     affiliation               url   institution
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                     &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt;      
## 1  4744 Tadepalli Prasad      Oregon State University   &amp;lt;NA&amp;gt;  NA         
## 2  4871 Black     Jessica Jan Saint Vincent College     &amp;lt;NA&amp;gt;  NA         
## 3  4477 Prull     Matthew     Whitman College           &amp;lt;NA&amp;gt;  NA         
## 4   753 Childers  Jane        Trinity University        &amp;lt;NA&amp;gt;  NA         
## 5  4881 Lindskog  Marcus      Uppsala University        &amp;lt;NA&amp;gt;  NA         
## 6  4852 Hudson    Danae       Missouri State University &amp;lt;NA&amp;gt;  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here is a very simple plot of the growth in authorized researchers and institutions over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(databraryapi::read_csv_data_as_df(), plot(Auth_Investigators, Institutions))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No encoding supplied: defaulting to UTF-8.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-28-test-driven-development-is-hard-and-important_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve said in other places that I think scientists will eventually interact with their data programmatically – via scripts like this – with the data and materials stored in repositories that others can also access programmatically. Furthermore, I think that the philosophy of test-driven development can help make our software AND the results and findings we derive from it more robust and reproducible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ethical data sharing</title>
      <link>/project/permission-to-share/</link>
      <pubDate>Thu, 27 Dec 2018 14:31:00 -0500</pubDate>
      
      <guid>/project/permission-to-share/</guid>
      <description>&lt;p&gt;Video recordings pose challenges for data sharing.
They contain faces and voices that cannot be easily eliminated or altered without harming the value of the recordings for other researchers.
My colleagues and I developed a data sharing policy framework for Databrary that requires researchers to gain consent to share from research participants and restricts access to institutionally-approved researchers.
Databrary&amp;rsquo;s template sharing release (permission) language may be found &lt;a href=&#34;https://www.databrary.org/resources/templates/release-template.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, and some draft language relevant for gaining permission to share identifiable in the context of the EU&amp;rsquo;s General Data Protection Regulation (GDPR) may be found &lt;a href=&#34;https://github.com/gilmore-lab/ethical-data-sharing-w-GDPR&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>databraryapi package</title>
      <link>/project/databraryapi/</link>
      <pubDate>Thu, 27 Dec 2018 12:52:01 -0500</pubDate>
      
      <guid>/project/databraryapi/</guid>
      <description>&lt;p&gt;I have released a working version (0.1.4) of an R package that interacts with the &lt;a href=&#34;https://databrary.org&#34; target=&#34;_blank&#34;&gt;Databrary.org&lt;/a&gt; API.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello, Hugo</title>
      <link>/post/getting-started/</link>
      <pubDate>Thu, 27 Dec 2018 12:37:00 -0500</pubDate>
      
      <guid>/post/getting-started/</guid>
      <description>&lt;p&gt;Hi, all.&lt;/p&gt;

&lt;p&gt;After a break, I&amp;rsquo;m back to blogging using the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt; package for R, and the Hugo &lt;strong&gt;Academic&lt;/strong&gt; theme.
Let me know what you think.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ending the endless war</title>
      <link>/post/ending-the-endless-war/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ending-the-endless-war/</guid>
      <description>&lt;p&gt;I find Andrew Sullivan worth reading almost all of the time even if I disagree with him.
This &lt;a href=&#34;http://nymag.com/intelligencer/2018/12/andrew-sullivan-establishment-will-never-say-no-to-a-war.html&#34; target=&#34;_blank&#34;&gt;time&lt;/a&gt;, I agree completely:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;&amp;hellip;I simply do not believe that the West has the knowledge, the will, or the ability to shape the extremely complicated and endlessly vicious politics of the Middle East. And I defy anyone to show otherwise&amp;hellip;&lt;/em&gt;&amp;ldquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, while I find the present occupant completely unfit for office, I applaud his announcement to withdraw U.S. troops from Syria and reduce our presence in Afghanistan.&lt;/p&gt;

&lt;p&gt;We forget that &lt;em&gt;Congress&lt;/em&gt; has the Constitutional power to declare war or decisively choose not to.
It&amp;rsquo;s part of the balance of powers created by the Framers.
I&amp;rsquo;d like to see Congress restore balance here and in other areas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Young children&#39;s neural processing of their mother’s voice: An fMRI study</title>
      <link>/publication/peep-ii-voices/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/peep-ii-voices/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neuroscience: The frontier within</title>
      <link>/talk/olli-2018-10-29/</link>
      <pubDate>Mon, 29 Oct 2018 00:00:00 -0400</pubDate>
      
      <guid>/talk/olli-2018-10-29/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Outdoors</title>
      <link>/personal/adventure/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/personal/adventure/</guid>
      <description>&lt;p&gt;For fun, I like to walk, run, hike, backpack, kayak and ride my bicycles: a foldable touring Bike Friday, a Specialized Hybrid that I sometimes use to commute to work, a Specialized Allez road bike, and stoker-willing, a road tandem we call Tonya.
Last summer, my wife and I took a tour of southern Tuscany with Vermont Bicycle Tours (VBT).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
